{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping Over Data Files\n",
    "\n",
    "---\n",
    "\n",
    "[Watch a walk-through of this lesson on YouTube]()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Questions:\n",
    "- How can I efficiently read in many data sets from different files?\n",
    "- How can I combine data from different files into one pandas DataFrame?\n",
    "\n",
    "## Learning Objectives:\n",
    "- Be able to write \"globbing\" expressions that match sets of files\n",
    "- Use `glob` to create lists of files\n",
    "- Write `for` loops to perform operations on many files \n",
    "- Write list comprehensions to perform operations on many files\n",
    "- combine pandas DataFrames\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Use a `for` loop to process files given a list of their names\n",
    "\n",
    "We can use a `for` loop to read in a set of data files, and do a thing for each one. In this case, we'll print the minimum value in each file:\n",
    "\n",
    "~~~python\n",
    "import pandas as pd\n",
    "\n",
    "data_files = ['data/gapminder_gdp_africa.csv', 'data/gapminder_gdp_asia.csv']\n",
    "\n",
    "for filename in data_files:\n",
    "    data = pd.read_csv(filename, index_col='country')\n",
    "    print(filename, data.min())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Use [`glob.glob`](https://docs.python.org/3/library/glob.html#glob.glob) to find sets of files whose names match a pattern.\n",
    "\n",
    "*   In Unix, the term ***globbing*** means matching a set of files with a pattern.\n",
    "*   The most common patterns are:\n",
    "    *   `*` meaning match zero or more characters\n",
    "    *   `?` meaning match exactly one character\n",
    "*   Python's standard library contains the [`glob`](https://docs.python.org/3/library/glob.html) module to provide pattern matching functionality\n",
    "*   The [`glob`](https://docs.python.org/3/library/glob.html) module contains a function also called `glob` to match file patterns\n",
    "*   E.g., `glob.glob('*.txt')` matches all files in the current directory \n",
    "    whose names end with `.txt`.\n",
    "*   Result is a list of strings.\n",
    "\n",
    "~~~python\n",
    "import glob\n",
    "print('all csv files in data directory:', glob.glob('data/*.csv'))\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "## Use `glob` and `for` to process batches of files.\n",
    "\n",
    "It's good practice to name your files systematically. As you've learned, Python is very precise about things like capitalization, so if your file names are inconsistent (e.g., `Gapminder_Europe.csv`, `gapminder_americas.csv`, `gapminder_Oceania.csv`), then it is harder to write code with `glob` that works correctly. \n",
    "\n",
    "For the Gapminder data, fortunatley the file names are quite systematic and consistent (as are the names of the columns inside each file), so we can use the following to read in each one and print the minimum GDP from 1952:\n",
    "\n",
    "~~~python\n",
    "for filename in glob.glob('data/gapminder_*.csv'):\n",
    "    data = pd.read_csv(filename)\n",
    "    print(filename, data['gdpPercap_1952'].min())\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending Files to a Single DataFrame\n",
    "\n",
    "Often we don't just want to open a file and extract a small bit of data (such as the minimum value in examples above). Rather, we might want to open a set of related data files and combine them into one big DataFrame. For example, in psychology and neuroscience most experiments involve multiple participants. For each participant, when we run the experiment we get a data file. To analyze the data across participants, we would want to read in all participants' data files and combined them into one DataFrame.\n",
    "\n",
    "pandas has a few methods that allow us to combine DataFrames, including:\n",
    "- [`.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "- [`.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html?highlight=merge#)\n",
    "- [`.append()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html?highlight=append#pandas.DataFrame.append)\n",
    "- [`.join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html#pandas.DataFrame.join)\n",
    "\n",
    "We will focus here on the first one. `concat` stands for \"concatenate\" which essentially means combine files by \"stacking\" them. That is, start with one DataFrame, and add a new data frame to the bottom of it, creating additional rows. In what we'll do here, we assume that all of the data files we're reading have the same columns. For example, in the Gapminder GDP data sets, each file has a columnt for `country` plus a series of columns for GDP in different years — and the same years are in the columns of all the data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading in data from multiple experimental participants\n",
    "\n",
    "Let's say we have data from an experiment in which we ran three human participants (sometimes called \"people\") on different days. For each participant, we have a data file. The columns in all the files are the same, because the files were generated by a computer program that ran the experiment.\n",
    "\n",
    "We give the participants anonymized ID codes to protect their privacy, and allow for a simple, systematic naming convention for the files. the first participant's data is saved in a file called `s1.csv`, the second's in `s2.csv`, etc..\n",
    "\n",
    "We can glob the data folder in which the files are stored, to find all the CSV files whose names start with an `s` followed by a single character, followed by `.csv`. We'll save the result to a list that we can loop through later:\n",
    "\n",
    "~~~python\n",
    "filenames = glob.glob('data/s?.csv')\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an empty list that we will store the DataFrames from each participant in. It will end up being a list of DataFrames (remember, lists can contain just about any other Python data type), and once we have read in all the data, we will combine them into one DataFrame. This is a trick that's important to use in pandas. The reason has to do with how pandas combines DataFrames and stores them in memory. In simple terms, each time we concatenate DataFrames, pandas does a lot of internal checking to make sure there are no errors. Doing this checking once, when combining many DataFrames, is far more efficient (and thus faster) than doing it many times. Likewise, when a DataFrame is created, an appropriate amount of memory space is allocated for it on the computer. Each time we append additional data, we have to create a new, bigger block of memory. Allocating new blocks of memory, many times, takes more time than just doing it once.\n",
    "\n",
    "~~~python\n",
    "df_list = []\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use a `for` loop to read the files in. This will cycle through the items in the `filenames` list; each time through the loop, `filename` has the value of the current file name, and we use the list `append()` method to add the data from that file to `df_list`:\n",
    "\n",
    "~~~python\n",
    "for f in filenames:\n",
    "    df_list.append(pd.read_csv(f))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we view the contents of the list, we see each data set, with its two columns (with headers saying what they are), and commas separating the list entries, as is typical of a list. \n",
    "\n",
    "~~~python\n",
    "df_list\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading multiple files using list comprehension\n",
    "\n",
    "While the `for` loop above works fine, there is an alternative way to do this, using [**list comprehension**](https://neuraldatascience.io/3/for-loops.html#list-comprehension). Recall that list comprehensions are basically just a compact version of a `for` loop, but they have some advantages:\n",
    "- they are *more pythonic*: they only require one line of code, whereas the `for` loop above required two\n",
    "- they are *more efficient*: list comprehensions actually run faster. This may not be an issue in the small examples here, but can make a big difference when working with real, large data sets\n",
    "\n",
    "~~~python\n",
    "df_list = [pd.read_csv(f) for f in filenames]\n",
    "df_list\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames\n",
    "\n",
    "At this point, we've read each input file in and stored it as a DataFrame, but we have a list of three distinct DataFrames. In most cases, we'll want to combine these in some way. Having built our list of DataFrames through reading a set of files, we can combine them into a single DataFrame using the pandas `.concat()` method:\n",
    "\n",
    "~~~python\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# Confirm this worked by viewing a random sample of rows\n",
    "df.sample(8)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the index column\n",
    "\n",
    "Recall that row labels in pands are called *indexes*. We can convert any column to an index using the `.set_index()` method. For this data, an approriate index is the participant ID, which is in the column `Participant`. Note that we need to assign the result of the `.set_index()` operation back to `df` for the change to be stored:\n",
    "\n",
    "~~~python\n",
    "df = df.set_index('Participant')\n",
    "df.sample(8)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "# Exercises\n",
    "## Determining Matches\n",
    "\n",
    "Which of these files is *not* matched by the expression `glob.glob('data/*as*.csv')`?\n",
    "\n",
    "1. `data/gapminder_gdp_africa.csv`\n",
    "2. `data/gapminder_gdp_americas.csv`\n",
    "3. `data/gapminder_gdp_asia.csv`\n",
    "\n",
    "```{admonition} Click the button to reveal the answer\n",
    ":class: dropdown\n",
    "\n",
    "1 and 2 are not matched. Only 3 is, because it is the only file name in which the string `as` occurs. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Globbing files\n",
    "\n",
    "Fill in the blanks so that the code below does the following: \n",
    "- Find all of the CSV files in the data folder that contain GDP data\n",
    "- Read these files in using a `for` loop\n",
    "- Concatenate the data files into a single pandas DataFrame\n",
    "- Print out the first 10 lines of the final combined DataFrame\n",
    "\n",
    "*Note* that not all the Gapminder data files contain GDP data, but the file names will indicate which ones do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "data_files = ___(___)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for ____ in ____:\n",
    "    df_list.___(pd.read_csv(f))\n",
    "    \n",
    "df = ___\n",
    "\n",
    "df.___(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "```{admonition} Click the button to reveal!\n",
    ":class: dropdown\n",
    "\n",
    "~~~python\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "data_files = glob.glob('data/*gdp*.csv')\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for f in data_files:\n",
    "    df_list.append(pd.read_csv(f))\n",
    "    \n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df.head(10)\n",
    "~~~\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension\n",
    "\n",
    "Now rewrite the code above to use list comprehension rather than a `for` loop, and only *two* lines of code total (excluding the `import` commands and viewing the first 10 lines of the result). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an even bigger challenge, see if you can reduce the code to a single line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Click the button to reveal!\n",
    ":class: dropdown\n",
    "\n",
    "Done in two lines of code:\n",
    "~~~python\n",
    "df_list = [pd.read_csv(f) for f in glob.glob('data/*gdp*.csv')]\n",
    "    \n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df.head(10)\n",
    "~~~\n",
    "\n",
    "Done in one line of code: \n",
    "\n",
    "~~~python\n",
    "df = pd.concat([pd.read_csv(f) for f in glob.glob('data/*gdp*.csv')])\n",
    "    \n",
    "df.head(10)\n",
    "~~~\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "## Summary of Key Points:\n",
    "- Use a `for` loop to process files given a list of their names\n",
    "- Use `glob.glob` to find sets of files whose names match a pattern\n",
    "- List comprehension can replace a `for` loop, resulting in more compact and efficient code\n",
    "- Naming your files in a consistent manner is just as important in data science, as writing the code to read them\n",
    "- When you want to combine multiple files into one pandas DataFrame, read each one in to a list of DataFrames, then run `pd.concat()` only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This lesson is adapted from the [Software Carpentry](https://software-carpentry.org/lessons/) [Plotting and Programming in Python](http://swcarpentry.github.io/python-novice-gapminder/) workshop. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
