{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pandas.pydata.org/static/img/pandas.svg\" width=200px>\n",
        "\n",
        "# pandas DataFrames\n",
        "---\n",
        "\n",
        "[Watch a walk-through of this lesson on YouTube](https://youtu.be/OJP-L_M1vEs)\n",
        "\n"
      ],
      "metadata": {},
      "id": "b4fe44d2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions:\n",
        "- How do I read in data from a file?\n",
        "- How can I work with data in tabular format (tables)?\n",
        "- How can I do basic descriptive statistics on tabular data?\n",
        "\n",
        "## Learning Objectives:\n",
        "- Select individual values from a Pandas dataframe\n",
        "- Select entire rows or entire columns from a dataframe\n",
        "- Select a subset of both rows and columns from a dataframe in a single operation\n",
        "- Select a subset of a dataframe by a single Boolean criterion\n",
        "- Obtain descriptive statistics for subsets of data within a table\n",
        "- Use the split-apply-combine paradigm to work with data\n",
        "---"
      ],
      "metadata": {},
      "id": "f9392fde"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is pandas?\n",
        "\n",
        "Bad news first: there are no cute, black-and-white bears here. [pandas](https://pandas.pydata.org/docs/) (whose official name starts with a lower-case \"p\") is a Python *library* for working with data in a tabular format, such as is found in file formats like CSV, Microsoft Excel, and Google Sheets. Unlike Excel or Sheets, pandas is not a point-and click graphical interface for working with these files — everything is done through Python code. But compared to other formats for working with data that we have seen in this workshop, such as lists and dictionaries, pandas may seem more familiar, and it definitely lends itself more naturally to large data sets. Indeed, pandas' mission statement is, \"...to be the fundamental high-level building block for doing practical, real world data analysis in Python\". \n",
        "\n",
        "The primary units of pandas data storage you will work with are [DataFrames](https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/01_table_oriented.html#min-tut-01-tableoriented) (essentially, tables of data organized as rows and columns). DataFrames are actually collections of pandas [Series](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html) objects, which can be thought of as individual rows or columns (or vectors, or 1D arrays). \n",
        "\n",
        "Among the things that make pandas so attractive are the powerful interface to access individual records of the table, proper handling of missing values, and relational-databases operations between DataFrames. As well, pandas functions and methods are written to work intuitively and efficiently with data organized in tables. Most operations are *vectorized*, which means that they will automatically apply to all values in a DataFrame or Series without the need to write `for` loops to execute the same operation on a set of cells.\n",
        "\n",
        "pandas is built on top of the [NumPy](https://numpy.org) library. Although we haven't discussed NumPy in this workshop, it is a powerful and widely used Python library for working with numerical data. However, it's worth noting for your future reference that most of the methods defined for NumPy Arrays also apply to Pandas Series/DataFrames."
      ],
      "metadata": {},
      "id": "5bc0b97d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Python Libraries\n",
        "\n",
        "pandas is an example of a Python library. A **library** is a collection of files (called *modules*) that contains functions for use by other programs. Libraries provide ways of extending Python's functionality in different ways. They may also contain data values (e.g., numerical constants), entire sample data sets, and other things. A library's contents are supposed to be related, although there's no actual way to enforce that.\n",
        "\n",
        "The Python [standard library](https://docs.python.org/3/library/) is an extensive suite of modules that comes with Python itself. Everything we've done so far in this workshop has been part of the standard library. Many additional libraries are available; CoCalc has a large number of extra libraries already installed.\n",
        "\n",
        "To use a library in a particular Jupyter notebook or other Python program, we must import it using the `import` statement, like this:\n",
        "\n",
        "~~~python\n",
        "import pandas\n",
        "~~~\n",
        "\n",
        "Once a library is imported, we can use functions and methods from it. But, for functions we have to tell Python that the function can be found in a particular library we imported. For example, pandas has a function to import data from CSV (comma-separated value) files, called `read_csv`. To run this command, we would need to type:\n",
        "\n",
        "~~~python\n",
        "pandas.read_csv()\n",
        "~~~\n",
        "\n",
        "Since some package names are long, and adding the name to every function can result in a lot of typing, Python also allows us to assign an *alias* — a shorter name — to a library when we import it. For example, the convention for pandas is to give it the alias `pd` like this:\n",
        "\n",
        "~~~python\n",
        "import pandas as pd\n",
        "~~~\n",
        "\n",
        "Then to read a CSV file we could use:\n",
        "\n",
        "~~~python\n",
        "pd.read_csv()\n",
        "~~~\n",
        "\n",
        "In the cell below, import pandas with the alias pd:"
      ],
      "metadata": {},
      "id": "28364a5f"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "c8535f09"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing data with pandas\n",
        "\n",
        "As noted, we can read a CSV file and use it to create a pandas DataFrame, with the funciton `pd.read_csv()`. [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) is a text format used for storing tabular data, in which each line of the file corresponds to a row in the table, and columns are separated with commas (\"CSV\" stands for \"comma-separated values\"). Often the first row of a CSV file will be the *header*, containing labels for each column. \n",
        "\n",
        "The Gapminder data is in CSV format, so let's load in one of the Gapminder datasets with the command below. Note that when we read in a DataFrame, we need to assign it to a variable name so that we can reference it later. A convention when working with pandas is to call the DataFrame `df`. This works fine if you only have one DataFrame to work with, although if you are working with multiple DataFrames it is a good idea to give them more meaningful names.\n",
        "\n",
        "The Gapminder data are stored in a subfolder called `data`, so as the argument to `pd.read_csv()` below we give the folder name folled by a slash, then the file name:\n",
        "\n",
        "~~~python\n",
        "df = pd.read_csv('data/gapminder_gdp_europe.csv')\n",
        "~~~"
      ],
      "metadata": {},
      "id": "432fc30e"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "c5a3113e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can view the contents of the DataFrame `df` by simply typing its name and running the cell. Note that, unlike most of the examples we've used in previous lessons, we *don't* use the `print()` function. Although it works, the result is not nicely formatted the way the output is if we just use the name of the data frame.\n",
        "\n",
        "That is, run this command: `df` — not `print(df)` — in the cell below."
      ],
      "metadata": {},
      "id": "526f1678"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3ee896a5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see that the rows are numbered in boldface, starting with 0 as is the norm in Python. This boldfaced, leftmost column is called the **index** of the DataFrame, and provides one way of accessing data by rows. Across the top, you'll see that the column labels are also in boldface. pandas is pretty smart about automatically detecting when the first row of a CSV file contains header information (column names)."
      ],
      "metadata": {},
      "id": "345a2637"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heads or Tails?\n",
        "\n",
        "We might want to \"peek\" at the DataFrame without printing out the entire thing, especially if it's big. We can see the first 5 rows of a DataFrame with the `.head()` method:\n",
        "\n",
        "~~~python\n",
        "df.head()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "1071ecf1"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "1afe365c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "...or the last 5 rows with `.tail()`:"
      ],
      "metadata": {},
      "id": "6f7e6edf"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "0e5b4fea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see a random sample of rows from the DataFrame with `.sample()`, giving it a numerical argument to indicate the number of rows we want to see:\n",
        "\n",
        "~~~python\n",
        "df.sample(10)\n",
        "~~~"
      ],
      "metadata": {},
      "id": "5a224865"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "ec232369"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `.head()` and `.tail()` methods also optionally take a numerical argument, if you want to view a different number of rows from the default of 5."
      ],
      "metadata": {},
      "id": "7d7811b3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing values in a DataFrame\n",
        "\n",
        "One thing we often want to do is access a single cell in a DataFrame, or a range of cells. Each cell is uniquely defined by a combination of its row and column locations. \n",
        "\n",
        "### Numerical indexing using `.iloc[]`\n",
        "\n",
        "pandas provides two ways of accessing cell locations. One is using the numerical positions in the DataFrame, using the convention of [row, column] — with [0, 0] being the top left cell in the DataFrame. So for a pandas DataFrame with 3 rows and 3 columns, the indices of each cell are as shown:\n",
        "\n",
        "|   | col 0  | col 1  | col 2  | col 3  |\n",
        "|---|--------|--------|--------|--------|\n",
        "| 0 | [0, 0] | [0, 1] | [0, 2] | [0, 3] |\n",
        "| 1 | [1, 0] | [1, 1] | [1, 2] | [1, 3] |\n",
        "| 2 | [2, 0] | [2, 1] | [2, 2] | [2, 3] |\n",
        "| 3 | [3, 0] | [3, 1] | [3, 2] | [3, 3] |\n",
        "\n",
        "\n",
        "Numerical indexing of DataFrames is done with the `.iloc[]` method. For example, to access the GDP value for Austria in 1952 — which is located in the second row, third column of our current DataFrame, we would use:\n",
        "\n",
        "~~~python\n",
        "df.iloc[1, 2]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "be9caa36"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "f787f907"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label-based indexing using `.loc[]`\n",
        "\n",
        "The other way to access a location in a DataFrame is by its index and column *labels*, using the `.loc[]` method. As noted earlier, in the DataFrame we imported, the indexes are currently numbers, which were created automatically when we imported the data. The `.loc[]` method doesn't work with numerical indexes (that's what `iloc` is for — and you can't mix, say, a numerical row index with a column label), but in the data set we imported, the first column of this CSV file is actually meant to be its index: while all other columns are data values (GDP, in type float), the first column identifies the country with which each row of data is associated. \n",
        "\n",
        "pandas has a method for setting an index column, `.set_index()`, where the argument (in the parentheses) would be the name of the column to use as the index. So here we want to run:\n",
        "\n",
        "~~~python\n",
        "df = df.set_index('country')\n",
        "~~~\n",
        "\n",
        "**Note** that we need to assign the result of this operation back to `df` (using `df = `), otherwise the change will not actually modify `df`.\n",
        "\n",
        "In the cell below, use the `.set_index()` method to set the index of `df` to `country`, and then view the DataFrame again to see how it has changed."
      ],
      "metadata": {},
      "id": "fdfa7f4c"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "7b4469e7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, if we knew which column we wanted to use as the index before loading in the data file, we could have included the argument `index_col=` in the `pd.read_csv()` command:\n",
        "\n",
        "~~~python\n",
        "df = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\n",
        "~~~\n",
        "\n"
      ],
      "metadata": {},
      "id": "e7746fbd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined the index, we can access the 1952 GDP value for Austria by its index and column names:\n",
        "\n",
        "~~~python\n",
        "df.loc['Austria', 'gdpPercap_1952']\n",
        "~~~"
      ],
      "metadata": {},
      "id": "db963af7"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "2c4e1ee3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use `:` on its own to mean all columns or all rows.\n",
        "\n",
        "Using Python's familiar slicing notatio (which we've previously used for strings and lists), we can use `:` with `.iloc[]` or `.loc[]`, to specify a range in a DataFrame.\n",
        "\n",
        "For example, to see the GDP of Albania for every year (column) in the DataFrame, we would use:\n",
        "\n",
        "~~~python\n",
        "df.loc[\"Albania\", :]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "347d116b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "003a1e33"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, we could see the GDP for every country (row) in the year 1957 with:\n",
        "\n",
        "~~~python\n",
        "df.loc[:, 'gdpPercap_1957']\n",
        "~~~"
      ],
      "metadata": {},
      "id": "0d7e7e6d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also just specify the row index; if you don't specify anything for the columns, pandas assumes you want all columns:\n",
        "\n",
        "~~~python\n",
        "df.loc[\"Albania\"]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "a7f8b81a"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "7f566a08"
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, since the syntax for `.iloc[]` and `.loc[]` is [rows, columns], you cannot omit a row index; you need to use `:` if you want all rows."
      ],
      "metadata": {},
      "id": "78f636c3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing works on DataFrames\n",
        "\n",
        "Slicing using numerical indices works similarly for DataFrames as we previously saw for strings and lists, for example, the following code will print the third through fifth rows of the DataFrame, and the fifth through eighth columns (remember, Python indexing starts at 0, and slicing does not include the \"end\" index): \n",
        "\n",
        "~~~python\n",
        "df.iloc[2:5, 4:8]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "a871efab"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "ee25b611"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will print from the sixth to second-last row of the DataFrame, and from the ninth to the last column:\n",
        "\n",
        "~~~python\n",
        "df.iloc[5:-1, 8:]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "488e8bad"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "87a0a875"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** however, that when using label-based indexing with `.loc[]`, pandas' slicing behaviour is a bit different. Specifically, the output *includes* the last item in the range, whereas numerical indexing with `.iloc[]` does not. \n",
        "\n",
        "So, considering that the first three rows of the DataFrame correspond to the countries Albania, Austria, and Belgium, and that columns 6 and 7 are for the years 1982 and 1987 respectively, compare the output of:\n",
        "\n",
        "~~~python\n",
        "df.iloc[0:2, 6:7]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "21fc0282"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3cf0509c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "with:\n",
        "\n",
        "~~~python\n",
        "df.loc['Albania':'Belgium', 'gdpPercap_1982':'gdpPercap_1988']\n",
        "~~~"
      ],
      "metadata": {},
      "id": "7053eebe"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "710c6adc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"inclusive\" label-based indexing with `.loc[]` is fairly intuitive, but it's important to remember that it works differently from numerical indexing."
      ],
      "metadata": {},
      "id": "189f599d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use lists to select non-contiguous sections of a DataFrame\n",
        "\n",
        "While slicing can be very useful, sometimes we might want to extract values that aren't next to each other in a DataFrame. For example, what if we only want values from two specific years (1992 and 2002), for Scandinavian countries (Denmark, Finland, Norway, and Sweden)? Nether these years nor countries are in adjacent columns/rows in the DataFrame. With `.loc[]`, we can use lists, rather than ranges separated by `:`, as selectors:\n",
        "\n",
        "~~~python\n",
        "df.loc[['Denmark', 'Finland', 'Norway', 'Sweden'], ['gdpPercap_1992', 'gdpPercap_2002']]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "d0736538"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "1f9d389a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can equivalently write the command over several lines to make it a bit easier to read:\n",
        "\n",
        "~~~python\n",
        "df.loc[['Denmark', 'Finland', 'Norway', 'Sweden'], \n",
        "       ['gdpPercap_1992', 'gdpPercap_2002']\n",
        "      ]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "bfe33529"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "cfbfa659"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could also define those lists as variables, and pass the variables to `.loc[]`. This might be useful if you were going to use the lists more than once, as well as for clarity:\n",
        "\n",
        "~~~python\n",
        "scand_countries = ['Denmark', 'Finland', 'Iceland', 'Norway', 'Sweden']\n",
        "years = ['gdpPercap_1992', 'gdpPercap_2002']\n",
        "df.loc[scand_countries, years]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "1562c15b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "c03fd757"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take this a step further, and assign the output of a `.loc[]` selection like this to a new variable name. This makes a copy of the selected data, stored in a new DataFrame (or Series, if we only select one row or column) with its own name. This allows us to later reference and use that selection. \n",
        "\n",
        "~~~python\n",
        "scand_data = df.loc[scand_countries, years]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "ab187c19"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "7d019938"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It's easy to do simple math and statistics in DataFrames\n",
        "\n",
        "We prevoiusly learned about methods to get simple statistical values out of a Python list, like `.max()`, and `.min()`. pandas includes these and many more methods as well. For example, we can view the mean GDP for Italy across all years (columns) with:\n",
        "\n",
        "~~~python\n",
        "df.loc['Italy'].mean()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "2f72ee79"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "6f8c07fd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or the largest GDP in 1977 with:\n",
        "\n",
        "~~~python\n",
        "df.loc[:, 'gdpPercap_1977'].max()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "f79bcc64"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "0730ae1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful method is `.describe()`, which prints out a range of descriptive statistics for the range of data you specify. Without any slicing it provides information for each column:\n",
        "\n",
        "~~~python\n",
        "df.describe()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "de0ac82b"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3dbaedb7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-Exercise\n",
        "In the cell below, use the `scand_countries` and `years` variables to view descriptive statistics for all Scandinavian countries in each year."
      ],
      "metadata": {},
      "id": "c680de2c"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "1977cca3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate cells based on conditions\n",
        "\n",
        "pandas allows an easy way to identify values in a DataFrame that meet a certain condition, using operators like `<`, `>`, and `==`. For example, let's see which countries in a list had a GDP over 10,000 in 1962 and 1992. The result is reported in Booleans (True/False) for each cell.\n",
        "\n",
        "~~~python\n",
        "countries = ['France', 'Germany', 'Italy', 'Spain', 'United Kingdom']\n",
        "df.loc[countries, ['gdpPercap_1962', 'gdpPercap_1992']] > 10000\n",
        "~~~"
      ],
      "metadata": {},
      "id": "1a3fb6d5"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "c78bf674"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select values or NaN using a Boolean mask.\n",
        "\n",
        "A DataFrame full of Booleans is sometimes called a *mask* because of how it can be used. A mask removes values that are not True, and replaces them with `NaN` — a special Python value representing \"not a number\". This can be useful because pandas ignores NaN values when doing computations. \n",
        "\n",
        "We create a mask by assigning the output of a conditional statement to a variable name:\n",
        "\n",
        "\n",
        "~~~python\n",
        "mask = scand_data > 30000\n",
        "~~~"
      ],
      "metadata": {},
      "id": "65984688"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "a89267fb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can apply the mask to the DataFrame to get only the values that meet the criterion:\n",
        "\n",
        "~~~python\n",
        "scand_data[mask]\n",
        "~~~"
      ],
      "metadata": {},
      "id": "4edef08f"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3de222c2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of how this might be used, the steps above would now allow us to find the lowest GDP value in each year, that was above 30,000:\n",
        "\n",
        "~~~python\n",
        "scand_data[mask].min()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "2326b407"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "1a487806"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split-Apply-Combine\n",
        "\n",
        "A common task in data science is to split data into meaningful subgroups, apply an operation to each subgroup (e.g., compute the mean), and then combine the results into a single output, such as a table or a new DataFrame. This paradigm was famously [described by Hadley Wickham in a 2011 paper](http://dx.doi.org/10.18637/jss.v040.i01).\n",
        "\n",
        "pandas provides methods and grouping operations that are very efficient (*vectorized*) for split-apply-combine operations. "
      ],
      "metadata": {},
      "id": "57bb0ddc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's say that we wanted to compare the average GDP for different regions of Europe, divided as northern, southern, eastern, and western. To do this, we first have to create lists defining the countries belonging to each of these regions:\n",
        "\n",
        "~~~python\n",
        "northern = ['Denmark', 'Finland', 'Iceland', 'Norway', 'Sweden']\n",
        "southern = ['Greece', 'Italy', 'Portugal', 'Spain']\n",
        "eastern = ['Albania', 'Bosnia and Herzegovina', 'Bulgaria', 'Croatia', \n",
        "            'Czech Republic', 'Hungary', 'Montenegro', 'Poland', 'Romania', \n",
        "            'Serbia', 'Slovak Republic', 'Slovenia']\n",
        "western = ['Austria',  'Belgium', 'France', 'Germany', 'Ireland', \n",
        "            'Netherlands', 'Switzerland', 'United Kingdom']\n",
        "~~~"
      ],
      "metadata": {},
      "id": "81f9eca1"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "c617a093"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can make a new column simply by using `.loc[]` with the rows specified by one of the lists we just defined, a column name that doesn't already exist (in this case, we'll call it \"region\"), then assigning a region label to that combination of rows and column. We need to do this separately for each region. Note that when we first create the new column (\"region\"), pandas fills it with NaN values in any rows that were not defined by the assignment. For example, in the code below, the first line will create the column \"region\", and fill it with \"northern\" for any row in the `northern` list, and `NaN` to every other row. \n",
        "\n",
        "~~~python\n",
        "df.loc[northern, 'region'] = 'northern'\n",
        "df.loc[southern, 'region'] = 'southern'\n",
        "df.loc[eastern, 'region'] = 'eastern'\n",
        "df.loc[western, 'region'] = 'western'\n",
        "~~~"
      ],
      "metadata": {},
      "id": "fdc15c40"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "5dda31af"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split\n",
        "\n",
        "Now we can use this \"region\" column to split the data into groups, using a pandas method called `.groupby()`\n",
        "\n",
        "~~~python\n",
        "grouped_countries = df.groupby('region')\n",
        "~~~"
      ],
      "metadata": {},
      "id": "69b9bc1a"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "b75b1928"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this step doesn't create a new DataFrame, it creates a special kind of pandas object that points to a grouping in the original DataFrame:\n",
        "\n",
        "~~~python\n",
        "type(grouped_countries)\n",
        "~~~"
      ],
      "metadata": {},
      "id": "9957fb96"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "4ae12267"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply\n",
        "\n",
        "Now that we have split the data, we can apply a function separately to each group. Here we'll compute the mean GDP for each region, for each year:\n",
        "\n",
        "~~~python\n",
        "mean_gdp_by_region = grouped_countries.mean()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "bc236315"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3d9639f6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine\n",
        "\n",
        "The combine step actually occurred with the *apply* step above — the result is automatically combined into a table of mean values organized by region. But since our *apply* step (`.mean()`) saved the result to a variable, we can view the resulting table as the output of the *combine* step:\n",
        "\n",
        "~~~python\n",
        "mean_gdp_by_region\n",
        "~~~"
      ],
      "metadata": {},
      "id": "cffddcaf"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "5e797211"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chaining\n",
        "\n",
        "In Python, **chaining** refers to combining a number of operations in one command, using a sequence of methods. We can perform the above split-apply-combine procedure in a single step as follows. Note that because we don't assign the output to a variable name, it is displayed as output but not saved.\n",
        "\n",
        "~~~python\n",
        "df.groupby('region').mean()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "93a5f0ea"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "d443aa9b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Exercises"
      ],
      "metadata": {},
      "id": "76bced0d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Individual Values\n",
        "\n",
        "Write an expression to find the Per Capita GDP of Serbia in 2007."
      ],
      "metadata": {},
      "id": "d7602c69"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "2a0a746f-40d1-475f-b851-dfb3899e9b00"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "print(df.loc['Serbia', 'gdpPercap_2007'])\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "c7124acd-39bf-459f-9ee1-ed6014a9f0ee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extent of Slicing\n",
        "\n",
        "1.  Do the two statements below produce the same output? (Hint: you might want to use the `.head()` method to remind yourself of the structure of the DataFrame)\n",
        "2.  Based on this, what rule governs what is included (or not) in numerical slices and named slices in Pandas?\n",
        "\n",
        "~~~python\n",
        "print(df.iloc[0:2, 0:2])\n",
        "print(df.loc['Albania':'Belgium', 'gdpPercap_1952':'gdpPercap_1962'])\n",
        "~~~"
      ],
      "metadata": {},
      "id": "dbf2ffc9"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "7a6ab979"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "### Answer\n",
        "No, they do not produce the same output! The output of the first statement is:\n",
        "\n",
        "~~~python\n",
        "        gdpPercap_1952  gdpPercap_1957\n",
        "country                                \n",
        "Albania     1601.056136     1942.284244\n",
        "Austria     6137.076492     8842.598030\n",
        "~~~\n",
        "\n",
        "The second statement gives:\n",
        "~~~python\n",
        "        gdpPercap_1952  gdpPercap_1957  gdpPercap_1962\n",
        "country                                                \n",
        "Albania     1601.056136     1942.284244     2312.888958\n",
        "Austria     6137.076492     8842.598030    10750.721110\n",
        "Belgium     8343.105127     9714.960623    10991.206760\n",
        "~~~\n",
        "\n",
        "Clearly, the second statement produces an additional column and an additional row compared to the first statement.  What conclusion can we draw? We see that a numerical slice, 0:2, *omits* the final index (i.e. index 2) in the range provided, while a named slice, `'gdpPercap_1952':'gdpPercap_1962'`, *includes* the final element.\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "b089c5b9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstructing Data\n",
        "\n",
        "Explain what each line in the following short program does:\n",
        "what is in `df1`, `df2`, etc.?\n",
        "\n",
        "~~~python\n",
        "df1 = pd.read_csv('data/gapminder_all.csv', index_col='country')\n",
        "df2 = df1[df1['continent'] == 'Americas']\n",
        "df3 = df2.drop('Puerto Rico')\n",
        "df4 = df3.drop('continent', axis = 1)\n",
        "df4.to_csv('result.csv')\n",
        "~~~"
      ],
      "metadata": {},
      "id": "e45a8b16"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "28bf18bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "## Solution\n",
        "Let's go through this piece of code line by line.\n",
        "~~~python\n",
        "df1 = pd.read_csv('data/gapminder_all.csv', index_col='country')\n",
        "~~~\n",
        "\n",
        "This line loads the dataset containing the GDP data from all countries into a dataframe called \n",
        "`df1`. The `index_col='country'` parameter selects which column to use as the \n",
        "row labels in the dataframe.  \n",
        "~~~python\n",
        "df2 = df1[df1['continent'] == 'Americas']\n",
        "~~~\n",
        "\n",
        "This line makes a selection: only those rows of `df1` for which the 'continent' column matches 'Americas' are extracted. Notice how the Boolean expression inside the brackets, `df1['continent'] == 'Americas'`, is used to select only those rows where the expression is true. Try printing this expression! Can you print also its individual True/False elements? (hint: first assign the expression to a variable)\n",
        "~~~python\n",
        "df3 = df2.drop('Puerto Rico')\n",
        "~~~\n",
        "\n",
        "As the syntax suggests, this line drops the row from `df2` where the label is 'Puerto Rico'. The resulting dataframe `df3` has one row less than the original dataframe `df2`.\n",
        "~~~python\n",
        "df4 = df3.drop('continent', axis = 1)\n",
        "~~~\n",
        "\n",
        "Again we apply the drop function, but in this case we are dropping not a row but a whole column. To accomplish this, we need to specify also the `axis` parameter (we want to drop the second column which has index 1).\n",
        "\n",
        "~~~python\n",
        "df4.to_csv('result.csv')\n",
        "~~~\n",
        "\n",
        "The final step is to write the data that we have been working on to a csv file. Pandas makes this easy with the `to_csv()` function. The only required argument to the function is the filename. Note that the file will be written in the directory from which you started the Jupyter or Python session.\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "12bcc61d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Indices\n",
        "\n",
        "Explain in simple terms what `idxmin` and `idxmax` do. When would you use these methods?\n",
        "\n",
        "~~~python\n",
        "data = pd.read_csv('data/gapminder_gdp_europe.csv', index_col='country')\n",
        "data.idxmin()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "88937e34"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "b759801b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "~~~python\n",
        "data.idxmax()\n",
        "~~~"
      ],
      "metadata": {},
      "id": "45f674b8"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "4d858993"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "## Solution\n",
        "For each column in `data`, `idxmin` will return the index value corresponding to each column's minimum;\n",
        "`idxmax` will do accordingly the same for each column's maximum value.\n",
        "\n",
        "You can use these functions whenever you want to get the row index of the minimum/maximum value and not the actual minimum/maximum value.\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "b6639fe3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice with Selection\n",
        "\n",
        "From the previous exercise, the Gapminder GDP data for Europe should be loaded in as `data`. Using this DataFrame, write an expression to select each of the following:\n",
        "\n",
        "- GDP per capita for all countries in 1982"
      ],
      "metadata": {},
      "id": "c104d262"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "49514949-9fea-4f26-b957-74bb0421f38e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "~~~python\n",
        "data['gdpPercap_1982']\n",
        "~~~\n",
        "```"
      ],
      "metadata": {},
      "id": "20e817c2-a61a-403b-ae70-593dfb977977"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GDP per capita for Denmark for all years"
      ],
      "metadata": {},
      "id": "1ea0161c"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "80a93f9a-5a2e-4eeb-84d9-0fb3ceef8da8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "~~~python\n",
        "data.loc['Denmark',:]\n",
        "~~~\n",
        "\n",
        "or\n",
        "\n",
        "~~~python\n",
        "data.loc['Denmark',:'gdpPercap_2007']\n",
        "~~~\n",
        "```"
      ],
      "metadata": {},
      "id": "77a4cbcc-1155-4fea-b54f-74e695d69f6f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GDP per capita for all countries for years *after* 1985"
      ],
      "metadata": {},
      "id": "c7cce2bf"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "6936d916-0ba1-4e13-84a0-cf319d77b69e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "data.loc[:,'gdpPercap_1985':]\n",
        "\n",
        "# Pandas is smart enough to recognize the number at the end of the column label \n",
        "# and does not give you an error, although no column named `gdpPercap_1985` actually \n",
        "# exists. This is useful if new columns are added to the CSV file later.\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "627f9c24-66a3-46aa-a8d0-0c9bf7ffb31a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GDP per capita for each country in 2007 as a multiple of GDP per capita for that country in 1952"
      ],
      "metadata": {},
      "id": "6b319fc0"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "4f419464-1b65-43ce-8313-3ebb7901f3cb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "```{admonition} Click the button to reveal!\n",
        ":class: dropdown\n",
        "\n",
        "data['gdpPercap_2007']/data['gdpPercap_1952']\n",
        "\n",
        "```"
      ],
      "metadata": {},
      "id": "a8a5ffd1-4feb-45f2-af79-87a6ce6e84d2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Summary of Key Points:\n",
        "- pandas DataFrames are a powerful way of storing and working with tabular (row/column) data\n",
        "- pandas columns and rows can have names\n",
        "- pandas row names are called *indexes* which are numeric by default, but can be given other labels\n",
        "- Use the `.iloc[]` method with a DataFrame to select values by integer location, using [row, column] format\n",
        "- Use the `.loc[]` method with a DataFrame to select rows and/or columns, using named slices\n",
        "- Use `:` on its own to mean all columns or all rows\n",
        "- Result of slicing can be used in further operations\n",
        "- Use comparisons to select data based on value\n",
        "- Select values or `NaN` using a Boolean mask\n",
        "- use split-apply-combine to derive analytics from groupings within a DataFrame"
      ],
      "metadata": {},
      "id": "69fdf9ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "This lesson is adapted from the [Software Carpentry](https://software-carpentry.org/lessons/) [Plotting and Programming in Python](http://swcarpentry.github.io/python-novice-gapminder/) workshop. \n",
        "\n",
        "Licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) 2021 by Aaron J Newman"
      ],
      "metadata": {},
      "id": "bf019711"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}