{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16add2eb-c6f7-40d1-b2b1-1b73d0265eb3",
   "metadata": {},
   "source": [
    "# Thinking About Data for Plotting\n",
    "\n",
    "Visualization is an incredibly powerful way to present data. However, to generate effective plots, you need to understand the structure and organization of your data. In particular, you need to consider the different **variables** present in your data, whether they are **continuous** or **categorical**, and whether they are **nested** (often called *repeated measures*). In this lesson we'll review these different types of data and some common approaches to plotting them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8efa6e-2f26-47d5-815d-7c6592d9b332",
   "metadata": {},
   "source": [
    "## Continuous Data\n",
    "\n",
    "Continuous data comprises most numeric data that you will encounter. Values can range continuously (i.e., in very small steps) across some range of values (possibly infinite, but typically with practical bounds when dealing with neuroscientific or psychological data). In Python, numeric data are typically stored as integers or floating point numbers. \n",
    "\n",
    "### Thought Question\n",
    "If you think about the Gapminder GDP dataset that we have worked with in previous lessons, what variables are continuous?\n",
    "\n",
    "```{admonition} Click the button to reveal the solution\n",
    ":class: dropdown\n",
    "\n",
    "- GDP is continuous. The values representing gross domestic product represent millions of dollars, and are stored as floating point numbers\n",
    "- Year is also, arguably, a numeric variable. The way it is reprepsented in the Gapminder datasets could be considered categorical (see below), but year itself is a continuous variable, most typically reprepsented as integers.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b32c12-9ebe-40a4-9651-6c9386802f5f",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "Categorical data comprises most data that is not numeric. For example, in a drug study someone might receive an experimental drug or a placebo — it's one or the other. Or in a language study, each participant might be classified as a native English speaker or someone who learned English as a second language. Categorical data can include data that have some degree of continuity. For example, Some people learn English from their parents as the only language they hear in the first year of their lives, whereas others may hear another language at home but learn English fluently from an early age, from other kids in the neighborhood. So in many cases, we *treat* data as categorical — often for convenience — even when there are subtleties that are lost.\n",
    "\n",
    "### Thought Question\n",
    "What variable(s) in the Gapminder GDP set are categorical?\n",
    "\n",
    "```{admonition} Click the button to reveal the solution\n",
    ":class: dropdown\n",
    "\n",
    "- Country is definitely categorical. Each country has its own GDP values\n",
    "- As noted above, year is arguably categorical, because in the Gapminder dataset, not all years are present so they are not truly continuous. However, we would typically still treat year as continuous in this context, because it is a unit of time\n",
    "- We also created a categorical, variable called \"region\" in one lesson, sorting countries into northern/southern/eastern/western Europe\n",
    "\n",
    "```\n",
    "\n",
    "Continuous data can sometimes be made categorical as well. For example, height is a continuous variable, but for convenience in a research study we might want to classify people as \"short\", \"medium\", or \"tall\" rather than their precise height in centimetres. In research with children, participants are often categorized into groups such as grades 1-2, grades 3-4, grades 5-6, instead of treating grade (or age) as a continuous variable. This process of turning continuous data into categorical is called **discretizing** (making discrete). It can be useful if the data you collect aren't as continuous as the possible range of values (e.g., children's academic knowledge and abilities are typically more related to their grade level than their chronological age), or for ease of generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df327f5-ccdf-4e10-abb8-a5e6984a6d85",
   "metadata": {},
   "source": [
    "## Plotting Continuous and Categorical Data\n",
    "\n",
    "Recognizing which variables are continuous or categorical is important, because in many cases you need to plot them differently. Continuous data lend themselves to things like continuous lines (e.g., regression lines) and scatterplots. In contrast, plotting categorical data typically involves different plot \"objects\" for each category, such as difeerent bars in a bar graph, or different lines in a line plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756c903-0181-465e-961a-a63e441c4fa8",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "\n",
    "A critical property of any data set is the **distribution** of values, also commonly called its **variance**. Variance is actually where *all the information* is. If every data point were the same, the data couldn't tell us anything. Data are only interesting, or informative, if it contains *meaningful variability*. Data science is founded on different ways of assessing variability and giving it meaning. Often we talk about *systematic variability*, in other words variability that we can attribute to some measurable, systematic difference. For example, if children in grades 1-2 have smaller vocabularies than children in grades 3-4, we can say that vocabulary size varies systematically according to some property of each child — specifically what grade they are in.\n",
    "\n",
    "A critical fact in data is that variance is always present, but is typically in part systematic — related to something we measure — and in part random (often called \"noise\"). Visualization is a useful tool in exploring data in order to identify possible systematic patterns in data. Critically, we need to visualize how the *distribution* of values of some measrurement vary with respect to some other variable. The fact that there is random variability in all data (at least, all neuroscience and psychology data) means that we can't just look at the average values of data points in, say, each of two groups of school children. Their averages may be numerically different, but the critical question is whether the typical variability in their values differs systematically between higher and lower grades. Plotting distirbutions of data is a useful way of comparing distirbutions. Commonly in neuroscience and psychology, we also use statistical tests to estimate our level of certainty that a difference is believable. We will come to statistical testing later, but for now we focus on making inferences based on visualization fo the data. Well-designed visualizations can actually provide the same level of precision and confidence as statsitical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792ab50-08fb-4081-b0ca-1d7a19eeb052",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nested Data\n",
    "\n",
    "Nested data occur when some measurements are \"nested\" or isolated inside other variables. This is very common in cognitive and neuroscience research, where we take many measurements from each individual participant (e.g., lots of experimental trials, recording from multiple electrodes). Nested data can be continuous or categorical, but the variable that they are nested inside is almost always categorical (e.g., individual people or animals). In educational research, data may be collected from children in different schools; in this case data may be nested within each child, but children are in turn nested inside schools. \n",
    "\n",
    "Recognizing nested data is important because, typically, there is less variability within an individual (or other nesting category) than between individuals (or other categories). As discussed above, in exporatory data analysis (EDA) — as well as in statistics — measures of variability are an important way that we make inferences about the data. \n",
    "\n",
    "For example, say we run a reaction time experiment with 100 trials, and 10 participants. We thus have 100 x 10 = 1000 data points. The variability is likely lower within an individual than between – one participant may be on average 150 ms faster than another, but each individual may only show a variation of +/- 25 ms in their personal average reaction times. In this case, if we compute the averate over all 1000 trials, without considering nesting structure, our measure of the average variance across the 1000 trials will be very low, because each individual contributes so many (similar) trials. But if we first averate across the 100 trials for each participant, then compute the variability between these averages, we will capture just the person-to-person difference. In this case we are most likely see higher variance, which reflects the true person-to-person variation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba89750-d9e1-4ece-9d3d-712349c02e3c",
   "metadata": {},
   "source": [
    "## Representations of Data\n",
    "\n",
    "After thinking about the structure (organization of data) at a conceptual level, we need to think about how this is actually stored in data files. Although this is an extensive subject, for now we will focus on data stored in pandas DataFrames, because pandas is a powerful tool and we've already started learning how to work with pandas. \n",
    "\n",
    "pandas DataFrames are a **two-dimensional** way of storing data. The two dimensions are rows and columns. But within this framework, we can store extremely complex data containing many values, and many variables as well. \n",
    "\n",
    "### Long- vs. Wide-Form Data\n",
    "\n",
    "Many software tools represent data in two-dimensional row-column format, including Microsoft Excel, Google Sheets, and SPSS. Across all of them, data can be stored in two ways.\n",
    "\n",
    "In **wide-form** data, each unit of obesrvation (such as a human or animal research participant) is associated with one measurement of each experimental variable. So each row in the DataFrame contains all the data from one participant, and columns represent individual measured variables, including different levels of the same variable. The Gapminder GDP data set is an example of wide-form data. Each row contains all the data associated with a particular country, and the columns represent all the different measurements we have. The data values in all of the columns are GDP in millions of dollars, and the different columns store data from different levels of the \"years\" variable..\n",
    "\n",
    "In contrast, **long-form** data requires fewer columns to represent a particular dataset, but more rows. Meausrements from a single source (e.g., experimental participant) are stored across several rows. Rather than each level of each variable having its own column, only two columns are required: one specifying the level of the variable, and one specifying the value of the measurement at that level fo the variable, for that participant. If we were to convert the Gapminder GDP data to long form, rather than having many columns for hte different years, we would have one column labelled \"year\", that contained different values of year (e.g., 1900, 1920, etc.) and a second column called \"GDP\" that contained the GDP values in millions of dollars. Thus each country would have as many rows in the DataFrame as there were years for which we had measurements. Long-form data is often used with nested (repeated measures) data. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
