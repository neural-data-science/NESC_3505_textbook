{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18febbcc-b716-4b4e-852d-f4659f9bbc00",
   "metadata": {},
   "source": [
    "# Group Analysis of ERP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820622e9-8699-4653-b0bf-6c67f1476ee7",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dccf85-2fb3-4791-b0c9-cf71aecd369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "mne.set_log_level('error')  # reduce extraneous MNE output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f8bc8-b997-4c9c-8ca2-d2c8f412f2bd",
   "metadata": {},
   "source": [
    "## Define parameters\n",
    "\n",
    "We define a list of experimental conditions; that's about the only parameter we need to define here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092b7ea-87be-47a6-8250-00212f076324",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['Control', 'Violation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1790f-4da7-4659-9f18-19c831dd4f54",
   "metadata": {},
   "source": [
    "## Find data files\n",
    "\n",
    "We list all the `-ave.fif` files (`Evoked` data sets) associated with this experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667062b-3483-4e49-a10c-3758ed1be845",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "data_files = glob.glob(data_dir + '/sentence_n400_p*-ave.fif' )\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1f20e-cf5e-4b80-81cf-453fc533f5e5",
   "metadata": {},
   "source": [
    "## Load data files\n",
    "\n",
    "In the previous lesson on averaging trials for an individual participant, we ended by saving the list of MNE `Evoked` objects (representing the averages across trials for each condition) in a single file for that participant. \n",
    "\n",
    "Here, we will load in the files from a group of participants. To work with them easily and efficiently in MNE, we will store them as a dictionary, where each key is a condition label, and each value is a list of `Evoked` objects (the data from that condition, with each participant's `Evoked` as a list item).\n",
    "\n",
    "Here when we `read_evokeds()` you will likewise get a list of `Evoked` objects. Lists are a bit tricky since they don't contain labels. In the previous lesson we addressed this by manually changing the `.comment` field of each `Evoked` object to clearly label it. As well, in the case of the present data (as is good practice in any data science pipeline), we used the same code to generate all of the Evoked files, so the two experimental conditions occur in the same order in each `-ave.fif` file. Since each participant's data was saved in the same order, index 0 in the list will always be the same condition for all participants. For this reason, we can use `enumerate()` to loop over conditions and build our dictionary of `Evoked` objects here.\n",
    "\n",
    "Note that we use list comprehension to build the list of Evoked objects for each condition, and we use the index from `enumerate()` (`idx`) to specify which condition (list item) we read from each participant's data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e793d0-9e22-4783-ae69-689702a1465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evokeds = {}\n",
    "\n",
    "for idx, c in enumerate(conditions):\n",
    "    evokeds[c] = [mne.read_evokeds(d)[idx] for d in data_files]\n",
    "\n",
    "evokeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67500f-5eb3-4dbb-80d6-cce081d30cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d7672ce-6d1a-47d2-b094-8b59d57ee5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare Evoked waveforms\n",
    "\n",
    "`mne.viz.plot_compare_evokeds()` will recognize a dictionary of Evoked objects and use the keys as condition labels. Furthermore, when it sees that each value in the dictionary is a list of Evoked objects, it will combine them and plot not only the mean across the list (i.e., across participants, for each condition), but also the 95% confidence intervals (CIs), representing the variability across participants. As we learned in the [EDA] chapter, CIs are a useful way of visually assessing whether different conditions are likely to be statistically significantly different. In this case, the plot shows evidence of an N400 effect. The difference between conditions is largest between ~350–650 ms, and the CIs are most distinct (overlapping little or not at all) between 500–650 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43815a-0a6b-4185-9c89-6e0cbda0fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plot parameters\n",
    "roi = ['C3', 'Cz', 'C4', \n",
    "       'P3', 'Pz', 'P4']\n",
    "\n",
    "color_dict = {'Control':'blue', 'Violation':'red'}\n",
    "linestyle_dict = {'Control':'-', 'Violation':'--'}\n",
    "\n",
    "\n",
    "mne.viz.plot_compare_evokeds(evokeds,\n",
    "                             combine='mean',\n",
    "                             legend='lower right',\n",
    "                             picks=roi, show_sensors='upper right',\n",
    "                             colors=color_dict,\n",
    "                             linestyles=linestyle_dict,\n",
    "                             title='Violation vs. Control Waveforms'\n",
    "                            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545c991-5ac1-4e75-b447-e0b7300d836a",
   "metadata": {},
   "source": [
    "## Differences\n",
    "\n",
    "As we did in the single-participant analysis, we can also create difference waves to more easily visualize the difference between conditions, and compare it to zero (i.e., no difference between conditions). \n",
    "\n",
    "In order to get CIs that reflect the variance across participants, we need to compute the *Violation-Control* difference separately for each participant. As we did for a single participant's data, we use `mne.combine_evoked()` for this, with a weight of `1` for *Violation* and `-1` for *Control*. The only difference here is we put this function in a loop over participants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e3194-f327-4fd3-9259-72748a1130a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_waves = []\n",
    "for i in range(len(data_files)):\n",
    "    diff_waves.append(mne.combine_evoked([evokeds['Violation'][i], evokeds['Control'][i]],\n",
    "                                          weights=[1, -1]\n",
    "                                         )\n",
    "                     )\n",
    "\n",
    "diff_waves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71527d3-c30d-447d-a003-aa3294e72b5c",
   "metadata": {},
   "source": [
    "## Plot difference waveform\n",
    "\n",
    "In making the difference waveform above, we simply created a list of `Evoked` objects. We didn't create a dictionary since there is only one contrast (*Violation-Control*), so no need to have informative dictionary keys for a set of different items (contrasts). This is fine, however MNE's  `plot_compare_evokeds()` is a bit picky when it comes to its inputs, and whether it draws CIs or not. Specifically, the [API](https://mne.tools/stable/generated/mne.viz.plot_compare_evokeds.html) reads:\n",
    "> If a list of Evokeds, the contents are plotted with their .comment attributes used as condition labels... If a dict whose values are Evoked objects, the contents are plotted as single time series each and the keys are used as labels. If a [dict/list] of lists, the unweighted mean is plotted as a time series and the parametric confidence interval is plotted as a shaded area.\n",
    "\n",
    "In other words, when the function gets a list of `Evoked` objects as input, it will draw each object as a separate waveform, but if it gets a dictionary in which each entry is a list of `Evoked` objects, if will average them together and draw CIs. Since we desire the latter behaviour here, in the command below we create a dictionary \"on the fly\" inside the plotting command, with a key corresponding to the label we want, and the value being the list of difference waves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4275fa3-bb55-49ac-9f09-2f34b12324ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast = 'Violation-Control'\n",
    "mne.viz.plot_compare_evokeds({contrast:diff_waves}, combine='mean',\n",
    "                            legend=None,\n",
    "                            picks=roi, show_sensors='upper right',\n",
    "                            title=contrast\n",
    "                            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27a44c-e5cc-4173-b0a2-8ec3c079455a",
   "metadata": {},
   "source": [
    "## Scalp topographic map\n",
    "\n",
    "As with `plot_compare_evoked()`, it's important to understand what type of data the `plot_evoked_topomap()` function needs in order to get it to work right. Whereas `plot_compare_evoked()` will average over a list of `Evoked` objects, `plot_evoked_topomap()` will only accept a single Evoked object as input. Therefore, in the command below we apply the `mne.grand_average()` function to the list of `Evoked` objects (**Grand averaging** is a term used in EEG research to refer to an average across participants; this terminology developed to distinguish such an average from an average across trials, for a single participant).\n",
    "\n",
    "Here we plot the topographic map for the average amplitude over a 200 ms period, centered on 400 ms (i.e., 300–500 ms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16769912-e339-4d04-bb5a-d090d6064e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            times=.500, average=0.200, \n",
    "                            title=contrast,\n",
    "                            size=3\n",
    "                           )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca56c14-c617-421b-9f07-538161b29909",
   "metadata": {},
   "source": [
    "We can add a few enhancements to the plot to make it more interpretable:\n",
    "- The `show_names` kwarg tells the function to plot the names of each channel rather than just dots at each location.\n",
    "- The `contours=False` kwarg turns off the dashed contour lines you can see above. This is a matter of personal preference, however I feel that these are like non-continuous colour scales, in that they provide visual indicators of discontinuous \"steps\" in the scalp electrical potential values, when in fact these vary smoothly and continuously. \n",
    "- We increase the `size` to make the channel labels a bit easier to read (unfortunately the function doesn't provide a kwarg to adjust the font size of these)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6edd2-79a5-45ad-953f-33b456ef6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_evoked_topomap(mne.grand_average(diff_waves), \n",
    "                            times=.500, average=0.200, \n",
    "                            title=contrast,\n",
    "                            show_names=True,\n",
    "                            contours=False,\n",
    "                            size=4\n",
    "                           );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154aa28-1f5a-4066-ba4d-537799bdd2de",
   "metadata": {},
   "source": [
    "## Statistical analyses\n",
    "\n",
    "### A priori *t* test\n",
    "A common approach to testing whether an ERP effect is significant is to perform a *t* test between a pair of conditions, on the ERP data averaged over a time period of interest and at one or a few electrodes. Ideally, the experimenter should have an *a priori* (i.e., defined prior to running the experiment) hypothesis regarding the time window and electrodes at which the ERP component and experimental effect of interest would be largest. In the present case, we predicted an N400, which we knew from previous studies with similar stimuli would likely be largest between 400–600 ms, at electrode Pz. \n",
    "\n",
    "In this case, we will use the `ttest_1samp()` function from SciPy's `stats` module. This tests whether a set of values are different from zero. So we use as input the *Violation-Control* differences for each participant, since if they are significantly different from zero, that will be evidence for a difference in amplitude between these two conditions. \n",
    "\n",
    "So here we first compute the average over the 400–600 ms time window, for each participant, at electrode Pz, and store these in a NumPy array on which we then perform the *t* test. We create the NumPY array in a single line of code, but it's a fairly complex one:\n",
    "- we use list comprehension to loop over each participant (`for e in diff_waves`)\n",
    "- for each participant's `Evoked` object, we use the `.get_data()` method to extract the ERP data\n",
    "    - we use the `picks` kwarg to specify which electrode(s) we want. In this cases, we specify a single electrode, however we could pass a list of several electrodes if we wanted.\n",
    "    - we use the `tmin` and `tmax` kwargs to specify the time range over which we want to extract the ERP data\n",
    "- The result of `get_data` will be a NumPy array of values at each time point in the time range we specified. We apply `np.mean()` to the result of `get_data`, to average across the 400-600 ms time window. We pass the `axis=1` kwarg because the NumPy array will be 2D, with a row for each electrode (in this case, only 1 row), and columns for time. So `axis=1` tells `np.mean()` to average over columns (axis 1; time) and not rows (axis 0; electrodes).\n",
    "- Since we used list comprehension to loop across participants, we end up with a list of NumPy arrays (one for each participant). Thus the outermost (last applied) function below is `np.array()` to convert the list of values from each participant to a NumPy array, because `ttest_1samp()` requires a NumPy array as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f55c54-06f5-4c80-b15f-a6dcbfa09e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_data = np.array([np.mean(e.get_data(picks='Pz', tmin=.400, tmax=.600), axis=1) for e in diff_waves])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb3e6f-ba5f-4d80-a3a9-35cd6937ec09",
   "metadata": {},
   "source": [
    "Now we perform the *t*-test. Note that `ttest_1samp()` produces NumPy arrays for its outputs (which here we assign to `t` and `pval`), even when we only performed one *t*-test. So when we print the results on the last line, we need to use `t[0]` and `p[0]` rather than simply `t` and `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c6e1a-f94e-4509-ad57-814dbbc10e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "t, pval = stats.ttest_1samp(evoked_data, 0)\n",
    "print('Difference t = ', str(round(t[0], 2)), 'p = ', str(round(pval[0], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364ae81-2386-4337-96d8-ccc11364b2f6",
   "metadata": {},
   "source": [
    "## Permutation *t* test and multiple comparison correction\n",
    "\n",
    "Another approach is to perform a *t* test at *every* electrode, so that we can visualize a statistical map of values across the scalp. This has the advantage of potentially revealing the true scalp distribution of an experimental effect, rather than being limited to the channel(s) predicted a priori to show the effect. \n",
    "\n",
    "However, this increases our risk of false positives (Type I error). This is because when we perform a *t* test and use the resulting *p* value to determine statistical significance, we typically pick a threshold for significance (alpha, or $\\alpha$) based on the probability of finding a result by chance, if no experimental effect were actually present. In other words, if we use an $\\alpha$ of .05, this means there is on average a 5% probability of finding that result by chance.  However, that is based on the assumption that we ran only one statistical test. If we run multiple statistical tests, and use *p* ≤ .05 as our threshold, then we increase the odds of finding at least one \"significant\" result by chance. Most intuitively, if we perform 100 statistical tests and use *p* ≤ .05 on each, then we are likely to end up with 5 of those results being incorrect (5/100 = 5%).\n",
    "\n",
    "In the present data we have data from 64 electrodes for each participant. One way to correct for multiple comparisons is the **Bonferroni** method, which involves simply dividing the desired $\\alpha$ value for the experiment (i.e., across all tests) by the number of tests. For this data set that would mean $.05 / 64 = 0.00078$. However, the Bonferroni correction assumes that each statistical test is independent of the others, which is not true for multiple channels of EEG data — nearby channels can be predicted to have similar values, since they are measuring activity from nearby scalp locations. \n",
    "\n",
    "There are several alternative ways of controlling for multiple comparisons. One is to perform a **permutation** *t* test. Widely introduced to the field of neuroimaging by [Nichols and Holmes (2001)](https://doi.org/10.1002/hbm.1058)permutation is a well-established approach in the field of statistics. The basic concepts in permutation testing are *exchangability* and *randomization*. If we have multiple data points from two sets of experimental conditions (e.g., *Violation* and *Control*), and there is no true difference between the conditions, then it should not matter if we perform a *t* test between the data points using the correct condition labels, or if we randomly re-label the condition of each data point (i.e., *exchange* the labels among data points). In other words, if ERP amplitude in the *Violation* and *Control* conditions are not truly different, then if we scramble which data points we call *Violation* and which ones we call *Control*, and perform the *t* test again, the results of that *t* test should not be too different from the \"correct\" *t* test. On the other hand, if there is a true experimental effect, then randomizing the labels of each data point should result in consistently lower *t* values than the true labeling. The reason this approach is called *permutation* testing is that we perform the random exchange of labels and data points many times, each with a different reassignment (i.e., a different permutation of labels and data points). Typically this is done thousands of times, in order to get a reliable estimate of the *null distribution* — that is, the distribution of *t* scores across different permutations that we know don't match the true label-to-data-point mapping. \n",
    "\n",
    "This approach doesn't in and of itself correct for multiple comparisons. However, when permutation tests are performed over a number of different sets of data (such as a separate *t* test on the data from each individual electrode), can can use this to correct for multiple comparisons. This is called the **maximal statistic** method. It is based on the recognition that the distribution of *t* (and associated *p*) values across thousands of random permutations of labels actually reflects the \"chance\" that our use *p* values are based on. In other words, in conventional *t* tests if we use an $\\alpha$ of .05, then we are saying that we accept a 5% chance that over many replications of the experiment we would get a \"significant\" result by chance. Since our permutation distribution actually *is* thousands of replications of an \"experiment\" in which there is no true effect, we can say that the top 5% of the *t* values we obtain from the permutations are what we consider \"significant\". We can provide even better control for multiple comparisons by looking at the maximum *t* values not just at each electrode separately, but across all the electrodes. We can set the critical *t* value (i.e., the *t* threshold for significance) to be $(\\alpha * N) + 1$, where $N$ is the number of electrodes. That is, our *t* threshold is set so that just under 5% of the randomly-obtained *t* values are greater than this threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23dba1-65fa-4567-bc57-111bd4fad9b9",
   "metadata": {},
   "source": [
    "### Performing a permutation *t* test\n",
    "\n",
    "MNE provides a `permutation_t_test()` function which we import here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14f1b-83a7-445c-a7eb-2c50891e3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.stats import permutation_t_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19602f6-b7a5-4861-a728-00d16fb8e3cf",
   "metadata": {},
   "source": [
    "The `permutation_t_test()` function only allows for a 1-sample test, so — similar to the single-electrode *t* test above — we will run this on the *Violation-Control* differences for each participant. So we then create a NumPy array of data from the difference waves of each of our participants. The only difference from the previous example is that we don't use `picks` to select a channel; we extract data from all channels. We do average over our *a priori* time window of interest however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bce47-de2b-4a48-a97a-4d42d14470d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_data = np.array([np.mean(e.get_data(tmin=.400, tmax=.600), axis=1) for e in diff_waves])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556063d-7e35-4d94-b631-8e89f2abf788",
   "metadata": {},
   "source": [
    "To run the test, we have to specify the number of permutations, here we use MNE's suggested value of 50,000. We also use the `tail=-1` kwarg to specify a 1-tailed test that predicts a negative *t* score. This is because the N400 manifests as a greater negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31182d4c-2719-4d78-8947-c069f3e017c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 50000\n",
    "T0, p_values, H0 = permutation_t_test(evoked_data, n_permutations, tail=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f81dc0-3d79-45a5-9b3c-478495a5a84b",
   "metadata": {},
   "source": [
    "The result of `permutation_t_test()` is three NumPY arrays:\n",
    "- the *t* values for each channel\n",
    "- the *p* values for each channel (corrected for multiple comparisons)\n",
    "- `H0`, which is the null distribution — the maximum *t* value obtained during each permutation. This is the distribution that was used to determine the significance threshold. We don't actually need to use it, because the corrections are all done by the `permutation_t_test()`, but it's nice to know it's there if we want to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40c137-c248-497f-9bdd-24caeb28b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e54837-0109-4afd-b18c-a902f3d53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3793b-5e2f-4665-9786-ac7d78a69c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a610b9-7572-4951-b3d0-8c22c1ebf778",
   "metadata": {},
   "source": [
    "If we plot a histogram of the maximum *t* values from `H0`, we see that the bulk values are between 1–5, with a long tail extending out on the positive end to just above 8. Note that because of how the `permutation_t_test` function is written, these values are all positive, even though our actual predicted significant *t* values will be negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6164985-ac97-4ce3-bef0-d29fd20bd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(data=H0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae50cac-7aa5-47de-aaed-84bf633a882d",
   "metadata": {},
   "source": [
    "We can use a function `np.argpartition()` to sort the `H0` values and split them at a particular cutoff. This is specified in terms of indexes in the NumPy array, so if we want the top 5% of *t* values, we have to compute the split as the number of permutations (the length of `H0`) multiplied by .05, which will tell as that 5% of 50,000 is 2500. (If you want to understand the details of this code, see [this StackExchange post](https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array) and the [API for `np.argpartition()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argpartition.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3927c4a-c995-42be-9bdd-a54e6667e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = round(n_permutations * .05)\n",
    "split = np.argpartition(H0, -thresh)[-thresh:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad360fa-94dd-4530-b446-22c45d7ee789",
   "metadata": {},
   "source": [
    "Our permutation-corrected *t* threshold will be the lowest value in `split`, because `split` contains the largest 5% of *t* values in the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a0d08-d2fc-44fc-bc80-670a6517ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_thresh = sorted(H0[split])[0]\n",
    "print(t_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d4016-01e0-42af-9bd6-91e17843ef17",
   "metadata": {},
   "source": [
    "We can also plot the histogram just of this top 5% of *t* values. We set the *y* axis limit to be comparable to the full distribution of *t* values above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227eef01-f5c9-4e47-b028-30e90cefbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=H0[split])\n",
    "plt.ylim(0, 1600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13735b00-7da6-4ea4-aca7-45e03ceee5cb",
   "metadata": {},
   "source": [
    "We can apply a mask to the *t* values from each electrode to see which ones are significant at our threshold. We need to put the `-` sign in front of `t_thresh` since we are predicting negative *t* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fa6df-1565-44d2-acb4-08bdd4cf6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 < -t_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39994cc-2b3f-4f8c-af92-9d94176ccc05",
   "metadata": {},
   "source": [
    "We can see that there are 7 significant channels across the scalp. We can visualize them by plotting a topographic statistical map of the head, in which the colours reflect the *p* values, and significant electrodes are shown as white dots. \n",
    "\n",
    "To do this, we create a new `Evoked` object in which the data values are the *p* values from our permutation *t* test. The data we use in this `Evoked` are the negative log of the *p* values, which transforms the values so that the most significant *p* values become the largest values after transformation. This means that when we plot the *p* values, \"bigger\" means \"more significant\", which is the way we normally and intuitively interpret colour maps. \n",
    "\n",
    "The code below references `evokeds[conditions[0]][0].info` simply to copy the header from one of our actual evoked data sets to the newly-created one. This is important because to make a topographic plot, we'll need information like channel positions and possibly labels. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669fdedf-0869-4a1a-b80b-8a561f770903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evoked_p = mne.EvokedArray(-np.log10(p_values)[:, np.newaxis],\n",
    "                         evokeds[conditions[0]][0].info, tmin=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df446f-f70d-40bc-8e40-b89d459b1ad5",
   "metadata": {},
   "source": [
    "To highlight the significant electrodes, we will need a mask like the one above that is True where the *t* test was significant, and False elsewhere. We use `p_values[:, np.newaxis]` rather than just `p_values`, so that the mask is a 2D (1 x n_channels) array with the values in a single column, because this is how the `plot_topomap()` function requires the mask to be formatted. `np.newaxis` is a [tool in NumPy](https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis) for increasing the number of dimensions in an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5666ad-c94b-4bc2-9e04-27d5202f9fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = p_values[:, np.newaxis] <= 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3658cdab-7ef1-442f-876b-0d0cc9624aa3",
   "metadata": {},
   "source": [
    "Finally, we generate the topographic plot. A few new or unusual kwargs are worth noting:\n",
    "- Since this is a map of *p* values, \"time\" doesn't make sense, however the function requires a `times` value otherwise it defaults to plotting 4 time points (so in this case, 4 copies of the same plot)\n",
    "- `scalings=1` rounds the numerical values on the colour bar to 1 decimal place\n",
    "- the `Reds` cmap is used because the default Red-Blue map doesn't make sense for -log(p) values, which cannot be negative.\n",
    "- we use `vmin` to specify the minimum value for the colour scale to be 0, since the minimum value in the data is not zero, but it makes sense for the colourmap to use white to mean zero\n",
    "- the `mask` is used to highlight significant electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179fd37-1754-49f6-883d-3446d7a46c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evoked_p.plot_topomap(times=0, \n",
    "                    scalings=1,\n",
    "                    cmap='Reds', \n",
    "                    vmin=0,\n",
    "                    units='-log10(p)', \n",
    "                    mask=mask,\n",
    "                    size=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c572b-4695-49da-ae7d-bf42a94dc5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
